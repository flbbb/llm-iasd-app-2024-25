{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-8ltYHfbs5B",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from typing import List, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!curl -o imdb_dataset.csv 'https://cdn-lfs.hf.co/repos/77/fa/77fa70b48eef1c98bf08d7b3e43b710623c24c69b4f78d4484f43c3361e9d2af/dfc447764f82be365fa9c2beef4e8df89d3919e3da95f5088004797d79695aa2?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27IMDB%2520Dataset.csv%3B+filename%3D%22IMDB+Dataset.csv%22%3B&response-content-type=text%2Fcsv&Expires=1737389808&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzM4OTgwOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy83Ny9mYS83N2ZhNzBiNDhlZWYxYzk4YmYwOGQ3YjNlNDNiNzEwNjIzYzI0YzY5YjRmNzhkNDQ4NGY0M2MzMzYxZTlkMmFmL2RmYzQ0Nzc2NGY4MmJlMzY1ZmE5YzJiZWVmNGU4ZGY4OWQzOTE5ZTNkYTk1ZjUwODgwMDQ3OTdkNzk2OTVhYTI%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=MLN897M-A%7E9-FDaHic2d45v%7EVpjV8nwXsTQmPPFGfXHHKQx9FiJSbKC-cVxhSDxzLy7RzAqSa5%7EjjJTW1gKrrNvWDGqf0X%7EIWtir3blKp9Rl0MA7GIsqahmltAm0yOXwWpPxBnWoqJ6dmki7fDSoBrK3XFJygDGxpwmw2PPaEETrtxBRMPiI8IxF7QczEDNlHcqTdtPBuFaUkL-3MnYFcxqiCofanFVx789qvdxKAevTxu13E2TgbiJlq9rjQOI5dTLW-zW1lZxaSeaaAfkH4uk9emiORx%7EPXu5oI9jiv0oAPML38iH%7EGrc4Hb%7EXIFu8jKLRbaH1ezckvZ7hK2m%7EKA__&Key-Pair-Id=K3RPWS32NSSJCE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Course plan\n",
    "1. Machine learning for NLP\n",
    "2. Course + TP: Attention and transformers\n",
    "3. Course: Transformers and pre-training\n",
    "4. TP: Coding GPT\n",
    "5. TP: Coding GPT Part 2 + BERT\n",
    "6. Evaluation of LLMs\n",
    "\n",
    "2 homeworks.\n",
    "One graded TP.\n",
    "One project.\n",
    "\n",
    "# Evaluation\n",
    "- 25% Homework\n",
    "- 25% TP\n",
    "- 50% Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning for NLP\n",
    "\n",
    "ðŸš§ **Question** ðŸš§\n",
    "\n",
    "Why would one need NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "ðŸš§ **Question** ðŸš§\n",
    "\n",
    "Can you give two main domain of NLP applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# First application: text classification\n",
    "\n",
    "- Goal of text classification: assign a label to a text.\n",
    "- We actually want to model $p(y | x)$ where $x$ is the text and $y$ is the label.\n",
    "- Let's call $\\hat{p}$ our model. $\\hat{p}$ is a function that takes a text as input and outputs a label.\n",
    "\n",
    "ðŸš§ **Question** ðŸš§\n",
    "\n",
    "How would you model $\\hat{p}$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"imdb_dataset.csv\", nrows=5000)\n",
    "# Print some data. Make sure all text is printed with pandas, with word wrap\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"review\"]\n",
    "y = (df[\"sentiment\"] == \"positive\").astype(int)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=1234\n",
    ")\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhiteSpaceTokenizer:\n",
    "    # DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WhiteSpaceTokenizer()\n",
    "\n",
    "tokenizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.frequencies[746]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenizing a text means splitting it into words.\n",
    "\n",
    "This is not as easy as it seems. For instance, how would you tokenize the following text?\n",
    "\n",
    "```\n",
    "I'm a student.\n",
    "```\n",
    "\n",
    "- We can naively split on spaces.\n",
    "- We can use a library like `nltk`, that incorporates more rules to split the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(\"Hello world. I'm Florian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    # DM\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_text = \"Ã   lÃ®ved  in    San-FranÃ§isco...  ! for 12 years.\"\n",
    "preprocess_text(noisy_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(preprocess_text)\n",
    "X_test = X_test.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WhiteSpaceTokenizer()\n",
    "tokenizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhiteSpaceTokenizer:\n",
    "    def __init__(self):\n",
    "        # The vocabulary will store the mapping between text tokens and their id.\n",
    "        self.vocab = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "        # We will keep track of the number of times a word appears in the corpus.\n",
    "        self.frequencies = {}\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Converts the text to a list of tokens (substrings).\"\"\"\n",
    "        return word_tokenize(text)\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Take a text as input and return its associated tokenization, as a list of ids.\"\"\"\n",
    "        list_tokens = self.split_text(text)\n",
    "        list_ids = []\n",
    "        for token in list_tokens:\n",
    "            list_ids.append(self.vocab.get(token, -1))\n",
    "        return list_ids\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        tokens = [self.id_to_token[i] for i in ids]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def fit(self, corpus: List[str]):\n",
    "        \"\"\"Fits the tokenizer to a list of texts to construct its vocabulary.\"\"\"\n",
    "        current_id = 0\n",
    "        for text in tqdm(corpus):\n",
    "            list_tokens = self.split_text(text)\n",
    "            for token in list_tokens:\n",
    "                token_id = self.vocab.get(token, None)\n",
    "                if token_id is None:\n",
    "                    self.vocab[token] = current_id\n",
    "                    self.id_to_token[current_id] = token\n",
    "                    self.frequencies[current_id] = 0\n",
    "                    token_id = current_id\n",
    "                    current_id += 1\n",
    "\n",
    "                self.frequencies[token_id] += 1\n",
    "        self.num_words = current_id\n",
    "        self.vocab[\"<UNK>\"] = -1\n",
    "        self.id_to_token[-1] = \"<UNK>\"\n",
    "        print(f\"Built a vocabulary of {self.num_words} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i live in paris\"\n",
    "\n",
    "tokenizer = WhiteSpaceTokenizer()\n",
    "tokenizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i live in pÃ‚ris\"\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(\"List of tokens:\", token_ids)\n",
    "print(\"decoded tokens:\", tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTextClassifier:\n",
    "\n",
    "    def predict(self, text: str) -> int:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def predict_dataset(self, X: List[str]) -> List[int]:\n",
    "        return [self.predict(x) for x in X]\n",
    "\n",
    "    def evaluate(self, X: List[str], y: List[int]) -> Dict[str, float]:\n",
    "        predictions = self.predict_dataset(X)\n",
    "        y_array = np.array(y)\n",
    "        accuracy = np.mean(predictions == y_array)\n",
    "        return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountClassifier(BaseTextClassifier):\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "        self.tokenizer = WhiteSpaceTokenizer()\n",
    "\n",
    "    def get_word_count(self, X, y):\n",
    "        words_count = np.zeros((self.n_classes, self.tokenizer.num_words), dtype=int)\n",
    "        for i in range(len(X)):\n",
    "            tokens = self.tokenizer.encode(X[i])\n",
    "            for w in tokens:\n",
    "                if w != -1:\n",
    "                    words_count[y[i], w] += 1\n",
    "        return words_count\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tokenizer.fit(X)\n",
    "\n",
    "        word_count = self.get_word_count(X, y)\n",
    "        self.words_count = word_count\n",
    "\n",
    "    def predict(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Calculate a score for each label by summing\n",
    "        the normalized frequencies of the words in 'text'.\n",
    "        Then choose the label with the highest score.\n",
    "        \"\"\"\n",
    "        words = self.tokenizer.encode(text)\n",
    "        words = np.array([w for w in words if w != -1])\n",
    "        words_count_per_class = self.words_count[:, words]\n",
    "        scores = words_count_per_class.sum(axis=1)\n",
    "        return np.argmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.words_count[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = CountClassifier(2)\n",
    "classifier.fit(X_train, y_train)\n",
    "print(classifier.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.words_count[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyClassifier(BaseTextClassifier):\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "        # This attribute is not used, but kept for API consistency\n",
    "        self.tokenizer = WhiteSpaceTokenizer()\n",
    "\n",
    "    def get_word_count(self, X, y):\n",
    "        words_count = np.zeros((self.n_classes, self.tokenizer.num_words), dtype=int)\n",
    "        for i in range(len(X)):\n",
    "            tokens = self.tokenizer.encode(X[i])\n",
    "            for w in tokens:\n",
    "                if w != -1:\n",
    "                    words_count[y[i], w] += 1\n",
    "        frequencies = words_count / words_count.sum(axis=1, keepdims=True)\n",
    "        return words_count, frequencies\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tokenizer.fit(X)\n",
    "\n",
    "        word_count, frequencies = self.get_word_count(X, y)\n",
    "        self.frequencies = frequencies\n",
    "        self.words_count = word_count\n",
    "\n",
    "    def predict(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Calculate a score for each label by summing\n",
    "        the normalized frequencies of the words in 'text'.\n",
    "        Then choose the label with the highest score.\n",
    "        \"\"\"\n",
    "        global_count = self.words_count.sum(axis=0)\n",
    "        global_freq = global_count / global_count.sum()\n",
    "        words = self.tokenizer.encode(text)\n",
    "        words = np.array([w for w in words if w != -1])\n",
    "        words_count_per_class = self.frequencies[:, words] / global_freq[words]\n",
    "        scores = words_count_per_class.sum(axis=1)\n",
    "        return np.argmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = FrequencyClassifier(2)\n",
    "classifier.fit(X_train, y_train)\n",
    "print(classifier.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸš§ **Question** ðŸš§\n",
    "\n",
    "How can we optimize the algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_vec = vectorizer.fit_transform(X.apply(preprocess_text))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vec.toarray(), y, test_size=0.3, random_state=1234\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape  # (n, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyClassifier(BaseTextClassifier):\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "        # This attribute is not used, but kept for API consistency\n",
    "\n",
    "    def get_word_count(self, X, y):\n",
    "        words_count = np.zeros((self.n_classes, X.shape[-1]), dtype=int)\n",
    "        for label in range(self.n_classes):\n",
    "            words_count[label] = X[y == label].sum(axis=0)\n",
    "        frequencies = words_count / words_count.sum(axis=1, keepdims=True)\n",
    "        return words_count, frequencies\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        word_count, frequencies = self.get_word_count(X, y)\n",
    "        self.frequencies = frequencies\n",
    "        self.words_count = word_count\n",
    "        self.global_count = self.words_count.sum(axis=0) + 1\n",
    "        self.global_freq = self.global_count / self.global_count.sum()\n",
    "        self.global_freq = self.global_freq\n",
    "\n",
    "    def predict(self, x) -> int:\n",
    "        words_count_per_class = x * self.frequencies / self.global_freq\n",
    "        scores = words_count_per_class.sum(axis=1)\n",
    "        return np.argmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = FrequencyClassifier(2)\n",
    "classifier.fit(X_train, y_train)\n",
    "print(classifier.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vectorization and NLP\n",
    "\n",
    "- In Machine Learning, we need to work with vectors.\n",
    "- The challenge is to find meaningful vectors for texts.\n",
    "- We rely on things we can do easily:\n",
    "  - Counting words\n",
    "  - Counting n-grams\n",
    "  - Ratios between those counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag of words model\n",
    "\n",
    "- We count the number of times each word appears in a text.\n",
    "- We can then represent the text as a vector of counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frequency-based vectorization\n",
    " \n",
    "- We can normalize the counts by the total number of words in the text.\n",
    "- This gives us the frequency of each word in the text.\n",
    "- This is called the **term frequency**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TF-IDF\n",
    "\n",
    "- **Intuition**:\n",
    "  - If a word appears many times in a document, it is important.\n",
    "  - If a word appears in many documents, it is not very informative.\n",
    "  - For instance, the words \"the\", \"and\" appears in many documents.\n",
    "\n",
    "- **TF-IDF**:\n",
    "  - We multiply the term frequency by the inverse document frequency.\n",
    "  - Used a lot for information retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Term Frequency (TF)\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{f_{t, d}}{\\sum_{t' \\in d} f_{t', d}}\n",
    "$$\n",
    "Where:\n",
    "- $ f_{t, d} $: Frequency of term $ t $ in document $ d $.\n",
    "- $ \\sum_{t' \\in d} f_{t', d} $: Total number of terms in document $ d $.\n",
    "\n",
    "---\n",
    "\n",
    "### Inverse Document Frequency (IDF)\n",
    "$$\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{N + 1}{\\text{DF}(t) + 1}\\right) + 1\n",
    "$$\n",
    "Where:\n",
    "- $ N $: Total number of documents.\n",
    "- $ \\text{DF}(t) $: Number of documents containing the term $ t $.\n",
    "- Adding $ +1 $ in numerator and denominator prevents division by zero.\n",
    "\n",
    "---\n",
    "\n",
    "### TF-IDF\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_to_tfidf(word_count_matrix):\n",
    "\n",
    "    # Term Frequency (TF)\n",
    "    term_frequencies = word_count_matrix / word_count_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # Document Frequency (DF)\n",
    "    document_frequencies = np.count_nonzero(word_count_matrix > 0, axis=0)\n",
    "\n",
    "    # Inverse Document Frequency (IDF)\n",
    "    num_documents = word_count_matrix.shape[0]\n",
    "    idf = np.log((num_documents + 1) / (document_frequencies + 1)) + 1\n",
    "\n",
    "    # TF-IDF\n",
    "    tfidf_matrix = term_frequencies * idf\n",
    "\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: identifying similar documents\n",
    "\n",
    "- We can use TF-IDF to identify similar documents.\n",
    "- To quickly compare two documents, we can compute the cosine similarity between their TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vec = vectorizer.fit_transform(X.apply(preprocess_text))\n",
    "\n",
    "query = \"i like film with boats\"\n",
    "\n",
    "query_vec = vectorizer.transform([query])\n",
    "\n",
    "similarity = X_vec.dot(query_vec.T).toarray().flatten()\n",
    "most_similar_idx = np.argmax(similarity)\n",
    "\n",
    "print(X[most_similar_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Limitations of frequency-based vectorization\n",
    "\n",
    "- Each word is treated independently, regardless of its context.\n",
    "- The order of words is not taken into account.\n",
    "- Semantic information is not captured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From document vectors to word embeddings\n",
    "\n",
    "- **Intuitive example**: Suppose we are interested in classifying texts in 4 categories: \"World\", \"Sports\", \"Business\", \"Sci/Tech\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o agnews.json 'https://cdn-lfs.hf.co/repos/1c/c2/1cc270208f262b4321ee4f29ffabdcb8628e94f3f5ff6bdd56b06989d62694b8/68dea4cd08fc6fca98f9cd4f3b173b553f29c65641514505f886683c7a9215ac?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27train.jsonl%3B+filename%3D%22train.jsonl%22%3B&Expires=1737499439&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzQ5OTQzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8xYy9jMi8xY2MyNzAyMDhmMjYyYjQzMjFlZTRmMjlmZmFiZGNiODYyOGU5NGYzZjVmZjZiZGQ1NmIwNjk4OWQ2MjY5NGI4LzY4ZGVhNGNkMDhmYzZmY2E5OGY5Y2Q0ZjNiMTczYjU1M2YyOWM2NTY0MTUxNDUwNWY4ODY2ODNjN2E5MjE1YWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=njTBsmkbfQWyCsrsVxIs-qCUJ97QtBDGWdoqGBbhbj5Vbwy2SkrpYDLQGv8IwFLbqyAXH15LF47N71L-GZ2fhGTowfPhMEt6jGT8XHjN-TSSawZ4iIO3PNRTZo8m1HzVgwzKzdTu6VUPtAvI5JcmOj4zRWlm9PhYRzfFUnMLcegvIZpYgY-NHpWvFN%7EVKuJQRTYXu%7ElqHnBO%7E8V0RydHsMcWPmMudgBJL6PVrbc1EK-5%7Er7EiUsm-ncyigAvRa0I4pbw6lLn3qXfETzlnCeYe2Q50bs-uz9Ar-LJM0t5fBGpsBNS8BrIpKsnMpYfdY7pOj0YqK3l5En4bZ1%7EOLp1iA__&Key-Pair-Id=K3RPWS32NSSJCE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(\"agnews.json\", lines=True, nrows=5000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word embeddings\n",
    "- We could manually design a vector for each word.\n",
    "- For instance, over the dimensions [World, Sported, Business, Sci/Tech]:\n",
    "  - \"Football\" could be [0, 1, 0, 0]\n",
    "  - \"Microsoft\" could be [0, 0, 0.5, 0.5]\n",
    "  - \"Olympics\" could be [0.25, 0.75, 0, 0]\n",
    "- But doing this naively would be very inefficient.\n",
    "  - Hard to design the vectors.\n",
    "  - Hard to compute them\n",
    "  - Does not account for the context of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning word embeddings\n",
    "\n",
    "- Instead, we can learn those embeddings from the data!\n",
    "- Same idea than with neural networks: we learn the weights of the embeddings from the data.\n",
    "\n",
    "**In practice**\n",
    "- Set a dimension $d$ for the embeddings (e.g. 100).\n",
    "- For each word, initialize a random vector of size $d$.\n",
    "- Train a model to predict something from the embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class WordEmbeddingClassifier(BaseTextClassifier):\n",
    "    def __init__(self, n_classes, voc_size, d_embed, tokenizer, lr=1e-3, n_epochs=10):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.voc_size = voc_size\n",
    "        self.lr = lr\n",
    "        self.d_embed = d_embed\n",
    "        self.n_epochs = n_epochs\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.embedding = nn.Embedding(voc_size, d_embed)\n",
    "        self.fc = nn.Linear(d_embed, n_classes)  # Output layer for classification\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(\n",
    "            list(self.embedding.parameters()) + list(self.fc.parameters()), lr=self.lr\n",
    "        )\n",
    "\n",
    "        # Tokenize and convert to tensors\n",
    "        X = [self.tokenizer.encode(text) for text in X]\n",
    "        X = [torch.tensor(tokens) for tokens in X]  # Convert to tensors\n",
    "        y = torch.tensor(y, dtype=torch.long)  # Convert labels to tensor\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in tqdm(range(self.n_epochs)):\n",
    "            epoch_loss = 0.0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for i, tokens in enumerate(X):\n",
    "\n",
    "                # Get embeddings and mean pooling\n",
    "                embedded = self.embedding(tokens)  # Shape: (seq_len, d_embed)\n",
    "                pooled = embedded.mean(dim=0)  # Shape: (d_embed,)\n",
    "\n",
    "                # Pass through classification layer\n",
    "                logits = self.fc(pooled.unsqueeze(0))  # Shape: (1, num_classes)\n",
    "\n",
    "                # Compute loss and backpropagate\n",
    "                loss = criterion(logits, y[i].unsqueeze(0))  # Shape: (1,)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.n_epochs}, Loss: {epoch_loss/len(X):.4f}\")\n",
    "\n",
    "    def predict(self, text: str) -> int:\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        tokens = torch.tensor([t for t in tokens if t != -1])  # Ignore invalid tokens\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedded = self.embedding(tokens)  # Shape: (seq_len, d_embed)\n",
    "            pooled = embedded.mean(dim=0)  # Shape: (d_embed,)\n",
    "            logits = self.fc(pooled.unsqueeze(0))  # Shape: (1, num_classes)\n",
    "            predicted_class = logits.argmax(dim=1).item()  # Get class index\n",
    "\n",
    "        return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"imdb_dataset.csv\", nrows=5000)\n",
    "# Print some data. Make sure all text is printed with pandas, with word wrap\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"review\"]\n",
    "y = df[\"sentiment\"]\n",
    "y = (y == \"positive\").astype(int)\n",
    "\n",
    "X = X.apply(preprocess_text)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=1234\n",
    ")\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WhiteSpaceTokenizer()\n",
    "tokenizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = WordEmbeddingClassifier(\n",
    "    voc_size=tokenizer.num_words,\n",
    "    n_classes=2,\n",
    "    d_embed=2,\n",
    "    tokenizer=tokenizer,\n",
    "    lr=0.01,\n",
    "    n_epochs=10,\n",
    ")\n",
    "\n",
    "classifier.fit(X_train.tolist(), y_train)\n",
    "\n",
    "print(classifier.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the embedding matrix of a review\n",
    "\n",
    "text = \"a very bad movie i hated it.\"\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens = torch.tensor([t for t in tokens if t != -1])  # Ignore invalid tokens\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedded = classifier.embedding(tokens)  # Shape: (seq_len, d_embed)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(embedded.T, cmap=\"viridis\", aspect=\"auto\")\n",
    "plt.xticks(\n",
    "    range(len(tokens)), [tokenizer.id_to_token[t.item()] for t in tokens], rotation=45\n",
    ")\n",
    "plt.ylabel(\"Embedding dimension\")\n",
    "\n",
    "plt.colorbar(label=\"Embedding value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word embeddings in practice\n",
    "\n",
    "- Word embeddings can be very powerful but costly to learn.\n",
    "- An innovative idea was to pre-train word embeddings on a large corpus.\n",
    "- This is the idea from the paper \"Distributed Representations of Words and Phrases and their Compositionality\" by Mikolov et al. (2013), which introduced the Word2Vec model.\n",
    "- \n",
    "### Word2Vec\n",
    "- Word2Vec is a contrastive learning model.\n",
    "- Key Idea: Words with similar contexts (neighboring words) have similar vector representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Word2Vec\n",
    "\n",
    "1. **Select a Word and Context**  \n",
    "   - Randomly sample a word $w$ from a document.  \n",
    "   - Define its **positive context** as the words within a window of size $R$ around $w$, excluding $w$ itself:  \n",
    "     $$\n",
    "     \\mathcal{C}_{\\text{pos}} = [w_{i-R}, \\dots, w_{i-1}, w_{i+1}, \\dots, w_{i+R}].\n",
    "     $$\n",
    "\n",
    "2. **Negative Examples**  \n",
    "   - To balance the training, introduce **negative examples** by sampling $2KR$ random words from the vocabulary $\\mathcal{V}$:  \n",
    "     $$\n",
    "     \\mathcal{C}_{\\text{neg}} = \\{c_1, \\dots, c_{2KR}\\}, \\quad c_j \\in \\mathcal{V}.\n",
    "     $$\n",
    "\n",
    "3. **Embedding Words**  \n",
    "   - Represent the target word $w$ as a vector $\\mathbf{v}_w \\in \\mathbb{R}^d$ using an embeddings table $\\mathcal{E}_w$.  \n",
    "   - Similarly, map each word in $\\mathcal{C}_{\\text{pos}}$ and $\\mathcal{C}_{\\text{neg}}$ to vectors:  \n",
    "     $$\n",
    "     \\mathbf{v}_{\\mathcal{C}_{\\text{pos}}} \\in \\mathbb{R}^{2R \\times d}, \\quad \\mathbf{v}_{\\mathcal{C}_{\\text{neg}}} \\in \\mathbb{R}^{2KR \\times d}.\n",
    "     $$\n",
    "\n",
    "4. **Compute Similarity**  \n",
    "   - For each context word $c$ in $\\mathcal{C}_{\\text{pos}} \\cup \\mathcal{C}_{\\text{neg}}$, compute the similarity with $w$ using the dot product:  \n",
    "     $$\n",
    "     s = \\mathbf{v}_c \\cdot \\mathbf{v}_w.\n",
    "     $$  \n",
    "   - Train the model to maximize the similarity between $w$ and the positive context words, and minimize the similarity with the negative context words.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llm-course-7UlZO0np-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
