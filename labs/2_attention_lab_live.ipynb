{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GN895Qhd_cU"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrlRLRGLd_cV"
      },
      "source": [
        "# Transformer from scratch\n",
        "\n",
        "In the two following sessions, we are going to code a Transformer model from scratch. For this first session, we focus on Attention for text classification. The goal is to develop a modular code, increasing the difficulty step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSMKQtcUd_cX"
      },
      "source": [
        "# Attention\n",
        "\n",
        "In this part, we will build an attention module, step by step. To have modular code, it is better to use `class` to code our module. We will mainly follow the course to get a good intuition of how attention works.\n",
        "\n",
        ":red_circle: **WARNING** :red_circle: **You are not allowed to use any existing pre-built torch module, like `nn.Linear`.** You can only use pytorch `Tensor` and the module you create.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkIyr8Bvd_cX"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MfTsl8Xd_cY"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VluqkjQrd_cY"
      },
      "outputs": [],
      "source": [
        "class_names = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "40f19f11ab724eb890fadfb342bb4ae9",
            "24721a58a2af48c0b4e9226beafecd2b",
            "29ab5eda8ad54135875c10108aaf7e97",
            "1f0b47d358fe4a1cb98be1714ff0fa29",
            "17f1ef99f375424fbfbc7cdcb0612857",
            "b9ca0ef12c5f4e5c8edc6950a6ca5432",
            "cda46e6a5fcf4c39a76f36d3890ec9c2",
            "c16309985c9a4d889808f872c4c2a876",
            "066040e388e74e4e9cdb8a1d484696e8",
            "2f9464fc113441b4b5a598d200442ab4",
            "fabba2e1cf25443f9606cf6bf96c27e0"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "9jFkgn6qd_cZ",
        "outputId": "a0442619-2f96-40d1-e756-2cdfd8c21e81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['label', 'title', 'description'],\n",
            "        num_rows: 700\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['label', 'title', 'description'],\n",
            "        num_rows: 300\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['label', 'title', 'description'],\n",
            "        num_rows: 300\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40f19f11ab724eb890fadfb342bb4ae9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "dataset = load_dataset(\"sh0416/ag_news\")\n",
        "dataset = dataset[\"train\"].select(range(1000))\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "dataset = dataset.train_test_split(test_size=0.3, seed=1234)\n",
        "dataset[\"validation\"] = dataset[\"test\"]\n",
        "\n",
        "print(dataset)\n",
        "\n",
        "\n",
        "def preprocess_text(x):\n",
        "\n",
        "    ids = tokenizer(x[\"description\"], truncation=True, max_length=256, padding=False)[\n",
        "        \"input_ids\"\n",
        "    ]\n",
        "    return {\"input_ids\": ids, \"label\": x[\"label\"] - 1}\n",
        "\n",
        "\n",
        "# Clean the dataset and tokenize it directly\n",
        "dataset = dataset.map(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGECcLjad_cZ"
      },
      "outputs": [],
      "source": [
        "class DataCollator:\n",
        "    def __init__(self, tokenizer, max_len=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # Tokenize the texts\n",
        "        labels = [example[\"label\"] for example in batch]\n",
        "        tokenized_texts = [example[\"input_ids\"] for example in batch]\n",
        "        # Pad the tokenized texts\n",
        "        max_len = max(len(text) for text in tokenized_texts)\n",
        "        padded_texts = [\n",
        "            text + [self.tokenizer.pad_token_id] * (max_len - len(text))\n",
        "            for text in tokenized_texts\n",
        "        ]\n",
        "        pad_mask = [\n",
        "            [1] * len(text) + [0] * (max_len - len(text)) for text in tokenized_texts\n",
        "        ]\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(padded_texts),\n",
        "            \"pad_mask\": torch.tensor(pad_mask),\n",
        "            \"labels\": torch.tensor(labels),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP0PSxxod_ca"
      },
      "outputs": [],
      "source": [
        "def validation_step(valid_dataloader, model, criterion):\n",
        "    n_valid = len(valid_dataloader.dataset)\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    n_iter = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "            labels = batch[\"labels\"].to(DEVICE)\n",
        "            pad_mask = batch[\"pad_mask\"].to(DEVICE)\n",
        "            output = model(input_ids, pad_mask)\n",
        "            loss = criterion(output, labels)\n",
        "            total_loss += loss.item()\n",
        "            correct += (output.argmax(axis=-1) == labels).sum().item()\n",
        "            n_iter += 1\n",
        "    return total_loss / n_iter, correct / n_valid\n",
        "\n",
        "\n",
        "def train_one_epoch(train_dataloader, model, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    n_train = len(train_dataloader.dataset)\n",
        "    n_iter = 0\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "        labels = batch[\"labels\"].to(DEVICE)\n",
        "        pad_mask = batch[\"pad_mask\"].to(DEVICE)\n",
        "        class_scores = model(input_ids, pad_mask)  # (B, 4)\n",
        "\n",
        "        loss = criterion(class_scores, labels)  # scalaire (1,)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (class_scores.argmax(axis=-1) == labels).sum().item()\n",
        "        n_iter += 1\n",
        "\n",
        "    return total_loss / n_iter, correct / n_train\n",
        "\n",
        "\n",
        "def train(model, train_dataloader, valid_dataloader, lr=0.01, n_epochs=5):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Track training loss, training accuracy, validation loss and validation accuracy and plot in the end\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    valid_losses = []\n",
        "    valid_accuracies = []\n",
        "    model.to(DEVICE)\n",
        "    for epoch in tqdm(range(n_epochs)):\n",
        "        train_loss, train_accuracy = train_one_epoch(\n",
        "            train_dataloader, model, optimizer, criterion\n",
        "        )\n",
        "        valid_loss, valid_accuracy = validation_step(valid_dataloader, model, criterion)\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1}: train_loss: {train_loss:.4f}, train_accuracy: {train_accuracy:.4f}, valid_loss: {valid_loss:.4f}, valid_accuracy: {valid_accuracy:.4f}\"\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        valid_losses.append(valid_loss)\n",
        "        valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label=\"train loss\")\n",
        "    plt.plot(valid_losses, label=\"valid loss\")\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accuracies, label=\"train accuracy\")\n",
        "    plt.plot(valid_accuracies, label=\"valid accuracy\")\n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHBNcZefd_cb"
      },
      "source": [
        "## Attention-based classification model\n",
        "\n",
        "The first step since we have access to text in a tokenized form is to use \"static\" word embeddings.\n",
        "\n",
        "### Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex6ItFpcd_cb"
      },
      "source": [
        "\n",
        "🚧 **TODO** 🚧\n",
        "\n",
        "Write a class named \"Embeddings\" that will take as input a list of ids and return associated vectors. The ids are integer and their range is given by the vocabulary size. This last quantity directly derives from the choice we made when building the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18umrC3od_cc",
        "outputId": "8b9425c9-3f77-4c4b-a45b-6accccc41e58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "a = torch.randn(4, 10)  # 4 embeddings de dimension 10\n",
        "indices = [[0, 2, 1], [1, 1, 3]]\n",
        "indices = torch.tensor(indices)\n",
        "indices.shape\n",
        "a[indices].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wR8itINd_cc"
      },
      "outputs": [],
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, vocab_size, d):\n",
        "        # TODO\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Parameter(torch.randn(vocab_size, d))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is a batch of ids (B, L)\n",
        "        return self.embeddings[x]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrBWGVuxd_cd",
        "outputId": "f1e66cdc-62de-4df8-8019-380b609cc8ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "embedding_model = Embeddings(1000, 10)\n",
        "embedding_model(indices).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU1MfbO2d_cd"
      },
      "source": [
        "### Attention\n",
        "\n",
        "Here we will write a 'AttentionBasedClassifier' `class` that will take as input a list of vectors and return a list of probabilities over the possible classes. The classes should be contextualized with the input vectors, using attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PrqDe-Td_cd"
      },
      "source": [
        "🚧 **TODO** 🚧\n",
        "\n",
        "Write a class named \"ClassAttention\" that will take as input a list of word vectors, contextualize the embeddings over the classes and return a list of probabilities over classes.\n",
        "\n",
        "**HINT** Split the architectures into modular blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3Igntgkd_ce"
      },
      "outputs": [],
      "source": [
        "class Linear(nn.Module):\n",
        "    def __init__(self, d_in, d_out, add_bias=True):\n",
        "        # TODO\n",
        "        super().__init__()\n",
        "        self.proj_matrix = nn.Parameter(torch.randn(d_out, d_in))\n",
        "        if add_bias:\n",
        "            self.bias = nn.Parameter(torch.randn(d_out))\n",
        "            self.add_bias = True\n",
        "        else:\n",
        "            self.add_bias = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_out = self.proj_matrix @ x.transpose(1, 2)\n",
        "        if self.add_bias:\n",
        "            x_out = x_out + self.bias[None, None, :]\n",
        "        return x_out.transpose(1, 2)\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.randn(d))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is (B, L, D)\n",
        "        x_norm = x - x.mean(axis=2, keepdim=True)  # x_mean should be (B, L)\n",
        "        var = x.var(axis=2, keepdim=True)\n",
        "        x_norm = x_norm / torch.sqrt(var + self.eps)\n",
        "        x_norm = x_norm * self.gamma[None, None, :]\n",
        "        return x_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour construire l'attention\n",
        "\n",
        "**Init**\n",
        "\n",
        "- définir les matrices de projection (paramètres), $Q, K, V \\in \\mathbb{R}^{d \\times d}$ (avec la class `Linear`).\n",
        "\n",
        "**Forward**\n",
        "- input vectors $x \\in \\mathbb{R}^{B\\times L\\times D}$\n",
        "- les 3 projections, on a $q, k, v\\in \\mathbb{R}^{B \\times L \\times D}$.\n",
        "- calcule la \"similarité\", $\\forall i \\in [1, L], \\forall j \\in [1, L], \\ s_{ij} = q_i ^T k_j$.\n",
        "- normaliser les scores (avec le softmax) $s_{ij} = \\dfrac{e^{s_{ij}}}{\\sum_{l=1}^L e^{s_{il}}}$\n",
        "- on calcule les vecteurs de sortie: $y_i = \\sum_{j=1}^L \\alpha_{ij} v_j$."
      ],
      "metadata": {
        "id": "Jbg3tbqxmyIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q = torch.randn(2, 3, 5)\n",
        "k = torch.randn(2, 3, 5)\n",
        "s = q @ k.transpose(1, 2)"
      ],
      "metadata": {
        "id": "M3fJPBnqsXqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = torch.randn(3, 5)\n",
        "k = torch.randn(3, 5)\n",
        "s = q @ k.T\n",
        "print(s.shape) # (3, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "rbSfTbwLta8U",
        "outputId": "5e5852f8-9149-4d95-fc6b-ffd288643c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-16-9ad19f0dd3c6>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-9ad19f0dd3c6>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    s = q @ k.T\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LwSzpr1d_cf"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.q_proj = Linear(d, d, add_bias=False)\n",
        "        self.v_proj = Linear(d, d, add_bias=False)\n",
        "        self.k_proj = Linear(d, d, add_bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.q_proj(x) # (B, L, D)\n",
        "        k = self.k_proj(x) # (B, L, D)\n",
        "        v = self.v_proj(x) # (B, L, D)\n",
        "\n",
        "        s_ij = q @ k.transpose(1, 2) # (B, L, L)\n",
        "\n",
        "        # optionnel\n",
        "        d = x.shape[2]\n",
        "        s_ij = s_ij / math.sqrt(d)\n",
        "\n",
        "        # a_ij = torch.exp(s_ij)\n",
        "        # a_ij = a_ij / a_ij.sum(dim=2, keepdim=True)\n",
        "\n",
        "        a_ij = torch.nn.functional.softmax(s_ij, dim=2) # (B, L, L)\n",
        "        y = a_ij @  v\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B = 4\n",
        "L = 10\n",
        "d = 8\n",
        "a = torch.randn(B, L, L)\n",
        "v = torch.randn(B, L, d)\n",
        "y = a @ v\n",
        "print(y.shape)\n",
        "# i = 1\n",
        "# attention_i = a[0, i] # (L)\n",
        "# y_i = (v[0] * attention_i[:, None]).sum(axis=0) # d\n",
        "# print(y_i.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwQke85dzsX8",
        "outputId": "f7b08224-396e-412b-c803-3f3539f936d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 10, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4tc8GARd_cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c827b79e-42f9-49b0-cda6-9ceeb025143c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 10, 8])\n"
          ]
        }
      ],
      "source": [
        "# Try the model on a simple input:\n",
        "model = Attention(d=d)\n",
        "x = torch.randn(B, L, d)\n",
        "output = model(x)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsM9x4quwCXP"
      },
      "source": [
        "🚧 **TODO** 🚧\n",
        "\n",
        "Update the code above such that it takes as argument an `attention_mask` for padding.\n",
        "\n",
        "Use this `attention_mask` to modify the attention computation such that padding tokens do not interfere in the computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLGUEeKgwA7f"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "INF = 1e10\n",
        "\n",
        "\n",
        "class MaskedAttention(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        # TODO\n",
        "\n",
        "    def forward(self, x, pad_mask):\n",
        "        # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXE2yszSd_cg"
      },
      "source": [
        "🔴 **TEST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niYyi0uXDpTL"
      },
      "outputs": [],
      "source": [
        "model = MaskedClassAttention(d=50, n_classes=4, vocab_size=10000)\n",
        "\n",
        "seq_len = 10\n",
        "x_without_pad = torch.randint(0, 10000, (1, seq_len))\n",
        "attention_mask = torch.ones(1, seq_len)\n",
        "out_without_pad = model(x_without_pad, attention_mask)\n",
        "\n",
        "x_with_pad = torch.randint(0, 10000, (1, 2 * seq_len))\n",
        "attention_mask = torch.ones(1, 2 * seq_len)\n",
        "attention_mask[:, seq_len:] = 0\n",
        "out_with_pad = model(x_with_pad, attention_mask)\n",
        "\n",
        "assert out_without_pad.shape == out_with_pad.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvzUIUsjd_ch"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "n_train = len(dataset[\"train\"])\n",
        "n_valid = len(dataset[\"test\"])\n",
        "data_collator = DataCollator(tokenizer)\n",
        "train_dataloader = DataLoader(\n",
        "    dataset[\"train\"], batch_size=batch_size, collate_fn=data_collator, shuffle=True\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    dataset[\"test\"], batch_size=batch_size, collate_fn=data_collator, shuffle=True\n",
        ")\n",
        "\n",
        "model = MaskedClassAttention(d=10, n_classes=4, vocab_size=len(tokenizer))\n",
        "train(\n",
        "    model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    valid_dataloader=valid_dataloader,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env-llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "40f19f11ab724eb890fadfb342bb4ae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24721a58a2af48c0b4e9226beafecd2b",
              "IPY_MODEL_29ab5eda8ad54135875c10108aaf7e97",
              "IPY_MODEL_1f0b47d358fe4a1cb98be1714ff0fa29"
            ],
            "layout": "IPY_MODEL_17f1ef99f375424fbfbc7cdcb0612857"
          }
        },
        "24721a58a2af48c0b4e9226beafecd2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9ca0ef12c5f4e5c8edc6950a6ca5432",
            "placeholder": "​",
            "style": "IPY_MODEL_cda46e6a5fcf4c39a76f36d3890ec9c2",
            "value": "Map: 100%"
          }
        },
        "29ab5eda8ad54135875c10108aaf7e97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c16309985c9a4d889808f872c4c2a876",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_066040e388e74e4e9cdb8a1d484696e8",
            "value": 300
          }
        },
        "1f0b47d358fe4a1cb98be1714ff0fa29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f9464fc113441b4b5a598d200442ab4",
            "placeholder": "​",
            "style": "IPY_MODEL_fabba2e1cf25443f9606cf6bf96c27e0",
            "value": " 300/300 [00:00&lt;00:00, 2138.08 examples/s]"
          }
        },
        "17f1ef99f375424fbfbc7cdcb0612857": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9ca0ef12c5f4e5c8edc6950a6ca5432": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cda46e6a5fcf4c39a76f36d3890ec9c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c16309985c9a4d889808f872c4c2a876": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "066040e388e74e4e9cdb8a1d484696e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f9464fc113441b4b5a598d200442ab4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fabba2e1cf25443f9606cf6bf96c27e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}