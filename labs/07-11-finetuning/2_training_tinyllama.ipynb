{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j86DqgZL0DKv"
      },
      "source": [
        "# Training generative models\n",
        "\n",
        "In this notebook, we are going to showcase quickly how to train large generative models.\n",
        "\n",
        "The goal of this lab is also to demonstrate that there exists a lot of pre-built methods that can automate common tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPYz3b8k2FBI"
      },
      "source": [
        "## Download requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrz3cr_W2OHH"
      },
      "source": [
        "First install a library necessary to run quantized models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "582BktNnz76l"
      },
      "outputs": [],
      "source": [
        "pip install -U accelerate==1.8.1 peft==0.15.0 bitsandbytes==0.46.0 transformers==4.52.4 datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGI3qVAS2cJ8"
      },
      "source": [
        "## Load a generative model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxYRcDiM3Drc"
      },
      "source": [
        "We are going to load a generative model. Here we make use of heavy optimization technique to make sure everything fits on Collab GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOWcL0LU3JYt"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, quantization_config=bnb_config, device_map=\"auto\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaylX_8eUIgW"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj-rohWW3sEN"
      },
      "source": [
        "ðŸš§ **TODO** ðŸš§\n",
        "\n",
        "Experiment with the generation of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXKueBQI3rxs"
      },
      "outputs": [],
      "source": [
        "text = \"Paris is the capital of\"\n",
        "DEVICE = \"cuda:0\"\n",
        "\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
        "outputs = model.generate(**inputs, max_new_tokens=200, temperature=1.0, do_sample=True)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0t2lFH7QkkG"
      },
      "source": [
        "## Training\n",
        "\n",
        "First load an instruction dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmTDX58FRM8s"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "def process_data(sample):\n",
        "    return tokenizer(sample[\"text\"])\n",
        "\n",
        "\n",
        "data = load_dataset(\n",
        "    \"tatsu-lab/alpaca\",\n",
        ")\n",
        "data = data.map(process_data, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RV7WuvRYZf5"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv0XQgN1Xc55"
      },
      "outputs": [],
      "source": [
        "print(data[\"train\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtvdDn0XXYka"
      },
      "source": [
        "Since this model is quite big, we are going to reduce the number of trainable parameters.\n",
        "\n",
        "To do that, we use **Parameter Efficient Fine-Tuning** (PEFT).\n",
        "\n",
        "In PEFT, we do not tune the full model. Only a small subsets of the parameters are trained, while the others are frozen (i.e., not updated during training).\n",
        "\n",
        "ðŸš§ **TODO** ðŸš§\n",
        "\n",
        "Explain why PEFT reduces the memory cost.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXZFxSqSQmCh"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "# This determines automatically which module can be used for quantized training inside the model\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6KYW_ifQuY0"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRmENLiicfBw"
      },
      "source": [
        "The cell below adds a small amount of extra-paramter to the model using Lora technique, that is described here: https://arxiv.org/abs/2106.09685"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXqf9mqAQ44i"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Use default config\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sspsxxn7dRAb"
      },
      "source": [
        "We now use the Trainer class of HuggingFace. On simple installations it can be very effective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcTqhPPATwHK"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=16,\n",
        "        gradient_accumulation_steps=1,\n",
        "        max_steps=100,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK9aujFIXMdu"
      },
      "outputs": [],
      "source": [
        "text = \"### Instruction:\\nPropose an outdoor activity.\\n\\n### Response:\\n\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
        "model.config.use_cache = True\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=200, temperature=1.0, do_sample=True)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
